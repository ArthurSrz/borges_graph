# Data Model: RAG Observability Comparison

**Feature**: 003-rag-observability-comparison
**Date**: 2025-12-23

## Entity Overview

```
┌─────────────────┐       ┌─────────────────────┐
│   Experiment    │       │  EvaluationQuestion │
│                 │──────▶│                     │
│ - id            │       │ - input (query)     │
│ - name          │  1:N  │ - expected_output   │
│ - created_at    │       │ - metadata          │
│ - config        │       └─────────────────────┘
└─────────────────┘                │
        │                          │
        │ 1:N                      │ 1:2
        ▼                          ▼
┌─────────────────┐       ┌─────────────────────┐
│ ExperimentTrace │       │   SystemResponse    │
│                 │◀──────│                     │
│ - question_id   │       │ - system_name       │
│ - dust_response │  1:1  │ - answer            │
│ - graphrag_resp │       │ - latency_ms        │
│ - scores        │       │ - status            │
└─────────────────┘       └─────────────────────┘
```

---

## Entity Definitions

### Experiment

A complete evaluation run comparing both RAG systems across the dataset.

| Attribute | Type | Description |
|-----------|------|-------------|
| id | string | Unique experiment identifier (auto-generated by OPIK) |
| name | string | Human-readable experiment name, e.g., "dust-vs-graphrag-2025-12-23" |
| created_at | datetime | Timestamp when experiment was created |
| config | dict | Configuration metadata (metrics used, parallel workers, etc.) |
| project_name | string | OPIK project name: "law_graphRAG" |
| dataset_name | string | Dataset used: "civic-law-eval" |

**Lifecycle**:
- Created when `evaluate()` is called
- Updated as each question is processed
- Finalized when all questions complete

---

### EvaluationQuestion

A single question from the evaluation dataset with its expected answer.

| Attribute | Type | Description |
|-----------|------|-------------|
| input | string | The civic law question to ask both systems |
| expected_output | string | The expected/reference answer for metric comparison |
| metadata | dict | Optional: source, difficulty level, topic category |

**Source**: Loaded from OPIK dataset "civic-law-eval"

**Example**:
```json
{
  "input": "Quels sont les droits fondamentaux du citoyen?",
  "expected_output": "Les droits fondamentaux incluent le droit à la liberté, l'égalité devant la loi, et la liberté d'expression.",
  "metadata": {"topic": "rights", "difficulty": "easy"}
}
```

---

### SystemResponse

The response from either Dust or GraphRAG for a single question.

| Attribute | Type | Description |
|-----------|------|-------------|
| system_name | string | "dust" or "graphrag" |
| answer | string | The response text from the system |
| latency_ms | float | Response time in milliseconds |
| status | string | "success", "timeout", "error: {message}" |
| raw_response | dict | Full API response for debugging |

**States**:
- `success`: Answer received within timeout
- `timeout`: Request exceeded 30-second limit
- `error: {message}`: Exception occurred

---

### ExperimentTrace

A complete record of one question's evaluation, stored in OPIK.

| Attribute | Type | Description |
|-----------|------|-------------|
| question_id | string | Reference to the EvaluationQuestion |
| dust_response | SystemResponse | Response from Dust agent |
| graphrag_response | SystemResponse | Response from GraphRAG API |
| scores | dict | Computed metric scores for both systems |
| metadata | dict | Timing, question metadata, etc. |

**OPIK Storage**: Traces are automatically logged via the `evaluate()` function.

---

### MetricResult

The computed score for a single response against the expected answer.

| Attribute | Type | Description |
|-----------|------|-------------|
| metric_name | string | "contains", "equals", "regex_match", "latency_ms" |
| value | float | Score value (0-1 for heuristics, ms for latency) |
| reason | string | Explanation of the score |
| system | string | "dust" or "graphrag" |

**Aggregation**: OPIK provides min, max, mean, std, median for each metric.

---

## Relationships

| From | To | Cardinality | Description |
|------|-----|-------------|-------------|
| Experiment | EvaluationQuestion | 1:N | An experiment evaluates all questions in dataset |
| Experiment | ExperimentTrace | 1:N | One trace per question in experiment |
| EvaluationQuestion | SystemResponse | 1:2 | Each question gets response from both systems |
| ExperimentTrace | MetricResult | 1:N | Multiple metrics computed per trace |

---

## Validation Rules

### Experiment
- `name` must be unique within project
- `config` must include at least one metric

### EvaluationQuestion
- `input` must be non-empty string
- `expected_output` should be present for metric computation

### SystemResponse
- `latency_ms` must be >= 0
- `status` must be one of defined states
- `answer` may be empty only if status is not "success"

### MetricResult
- `value` must be in valid range for metric type
- For heuristic metrics: 0 <= value <= 1
- For latency: value >= 0

---

## State Transitions

### Experiment Lifecycle
```
CREATED ──▶ RUNNING ──▶ COMPLETED
                │
                └──▶ FAILED (if critical error)
```

### SystemResponse Status
```
PENDING ──▶ SUCCESS
        │
        ├──▶ TIMEOUT (after 30s)
        │
        └──▶ ERROR (on exception)
```

---

## OPIK Data Mapping

This maps internal entities to OPIK SDK structures:

| Internal Entity | OPIK Equivalent | Notes |
|-----------------|-----------------|-------|
| Experiment | `EvaluationResult` | Returned by `evaluate()` |
| EvaluationQuestion | Dataset item | `{"input": ..., "expected_output": ...}` |
| ExperimentTrace | Trace | Auto-logged by OPIK |
| MetricResult | `ScoreResult` | Returned by metric's `score()` method |

---

## Example Data Flow

```
1. Load dataset
   └─▶ dataset.get_items() → [EvaluationQuestion, ...]

2. For each question:
   ├─▶ query_dust(question.input) → SystemResponse
   ├─▶ query_graphrag(question.input) → SystemResponse
   └─▶ compute_metrics(responses, question.expected_output) → [MetricResult, ...]

3. Log to OPIK
   └─▶ ExperimentTrace stored with all data

4. Aggregate results
   └─▶ Experiment.aggregate_evaluation_scores() → summary stats
```
